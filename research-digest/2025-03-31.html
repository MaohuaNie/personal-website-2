
    <html>
    <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="/assets/site.css">
    </head>
    <body style="
        font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif;
        background:#f6f7fb;
        padding:20px;
        color:#1f2937;
    ">
    
      <div style="
          max-width:900px;
          margin:0 auto;
          background:#ffffff;
          border-radius:14px;
          padding:26px;
          box-shadow:0 6px 18px rgba(0,0,0,0.08);
          border:1px solid #eef0f4;
      ">

        <div style="display:flex;align-items:flex-end;justify-content:space-between;gap:14px;flex-wrap:wrap;">
          <div>
            <h1 style="margin:0;font-size:26px;line-height:1.2;color:#111827;">
              Bi-weekly Research Digest
            </h1>
            <p style="margin:8px 0 0;font-size:14px;color:#4b5563;">
              <b>Date range:</b> 2025-03-16 → 2025-03-31<br>
              <b>Total relevant papers:</b> 4
            </p>
          </div>

        </div>

        <h2 style="
            margin:26px 0 10px;
            font-size:18px;
            border-bottom:1px solid #e5e7eb;
            padding-bottom:8px;
            color:#111827;
        ">
          Summary by Journal
        </h2>

        <ul style="margin:10px 0 0;padding-left:18px;color:#374151;line-height:1.7;">
    <li><b>Journal of Experimental Psychology: Learning, Memory, and Cognition</b>: 2</li><li><b>Psychological Review</b>: 1</li><li><b>Management Science</b>: 1</li></ul>
        <h2 style="
            margin:34px 0 10px;
            font-size:24px;
            color:#b91c1c;
        ">
          Journal of Experimental Psychology: Learning, Memory, and Cognition
        </h2>
        
            <div style="
                background:#ffffff;
                border:1px solid #e5e7eb;
                border-radius:14px;
                padding:18px;
                margin:14px 0 18px;
                box-shadow:0 2px 10px rgba(0,0,0,0.04);
            ">
              <h3 style="margin:0 0 10px;font-size:20px;color:#111827;">
                Frequency effects in human category learning.
              </h3>

              <p style='margin:0 0 12px;font-size:14px;color:#374151;line-height:1.6;'>The study finds frequency and recency effects in categorization, with a recency-weighted exemplar model best explaining performance and transfer, suggesting categorization decisions are accumulative rather than averaged.</p>

              <div style="font-size:13px;color:#4b5563;line-height:1.7;">
                <div><b>Authors:</b> Yang, D., Worthy, D.</div>
                <div><b>Published:</b> 2025-03</div>
                <div><b>Relevance Score:</b> 0.41</div>
                <div style="margin-top:10px;">
                  <b>DOI:</b>
                  <a href='https://doi.org/10.1037/xlm0001474' target="_blank" rel="noopener noreferrer" style='color:#2563eb;text-decoration:none;'>https://doi.org/10.1037/xlm0001474</a>
                  
                </div>

                <div style="
                    margin-top:12px;
                    padding:12px;
                    background:#f9fafb;
                    border:1px solid #eef0f4;
                    border-radius:12px;
                ">
                  <b>Abstract:</b><br>
                  <span style="color:#374151;">This study investigated the assumptions of prototype and exemplar models of human category learning, with a particular focus on the impact of category frequency. We used baseline and recency-weighted variants of prototype and exemplar models to examine the computational mechanisms underlying categorization decisions when one category was presented more frequently than the other. We employed extensive sets of stimuli derived from bivariate normal distributions and manipulated category frequency during training across four experiments using different category structures. In the transfer phases, participants classified novel stimuli. Across all studies, the results revealed a significant frequency effect, with participants showing a preference for categorizing novel items as members of the more frequently encountered category. This preference extended to transfer stimuli outside the trained region of the stimulus space. Model-based analyses indicated that the recency-weighted generalized context model exemplar model, which computes summed similarity via a Decay reinforcement learning rule, consistently outperformed other models in fitting the data and accurately reproducing the observed classification patterns across all experiments. Both prototype models failed to account for the observed frequency effects. While the baseline generalized context model was able to account for frequency effects, it did not capture recency effects. These findings suggest that relative category frequency influences human behavior when categorizing novel items. The computational modeling results revealed that evidence for categorization decisions is recency-weighted and accumulative rather than averaged. (PsycInfo Database Record (c) 2025 APA, all rights reserved).</span>
                </div>
              </div>
            </div>
            
            <div style="
                background:#ffffff;
                border:1px solid #e5e7eb;
                border-radius:14px;
                padding:18px;
                margin:14px 0 18px;
                box-shadow:0 2px 10px rgba(0,0,0,0.04);
            ">
              <h3 style="margin:0 0 10px;font-size:20px;color:#111827;">
                Fast and slow errors: What naming latencies of errors reveal about the interplay of attentional control and word planning in speeded picture naming.
              </h3>

              <p style='margin:0 0 12px;font-size:14px;color:#374151;line-height:1.6;'>In a speeded picture-naming task, the study tests whether lexical errors arise from premature selection or attentional lapses and analyzes the RT distributions of different error types to infer their origins.</p>

              <div style="font-size:13px;color:#4b5563;line-height:1.7;">
                <div><b>Authors:</b> Papoutsi, C., Tourtouri, E., Piai, V., Lampe, L., Meyer, A.</div>
                <div><b>Published:</b> 2025-03</div>
                <div><b>Relevance Score:</b> 0.32</div>
                <div style="margin-top:10px;">
                  <b>DOI:</b>
                  <a href='https://doi.org/10.1037/xlm0001472' target="_blank" rel="noopener noreferrer" style='color:#2563eb;text-decoration:none;'>https://doi.org/10.1037/xlm0001472</a>
                  
                </div>

                <div style="
                    margin-top:12px;
                    padding:12px;
                    background:#f9fafb;
                    border:1px solid #eef0f4;
                    border-radius:12px;
                ">
                  <b>Abstract:</b><br>
                  <span style="color:#374151;">Speakers sometimes produce lexical errors, such as saying &quot;salt&quot; instead of &quot;pepper.&quot; This study aimed to better understand the origin of lexical errors by assessing whether they arise from a hasty selection and premature decision to speak (premature selection hypothesis) or from momentary attentional disengagement from the task (attentional lapse hypothesis). We analyzed data from a speeded picture naming task (Lampe et al., 2023) and investigated whether lexical errors are produced as fast as target (i.e., correct) responses, thus arising from premature selection, or whether they are produced more slowly than target responses, thus arising from lapses of attention. Using ex-Gaussian analyses, we found that lexical errors were slower than targets in the tail, but not in the normal part of the response time distribution, with the tail effect primarily resulting from errors that were not coordinates, that is, members of the target&#39;s semantic category. Moreover, we compared the coordinate errors and target responses in terms of their word-intrinsic properties and found that they were overall more frequent, shorter, and acquired earlier than targets. Given the present findings, we conclude that coordinate errors occur due to a premature selection but in the context of intact attentional control, following the same lexical constraints as targets, while other errors, given the variability in their nature, may vary in their origin, with one potential source being lapses of attention. (PsycInfo Database Record (c) 2025 APA, all rights reserved).</span>
                </div>
              </div>
            </div>
            
        <h2 style="
            margin:34px 0 10px;
            font-size:24px;
            color:#b91c1c;
        ">
          Psychological Review
        </h2>
        
            <div style="
                background:#ffffff;
                border:1px solid #e5e7eb;
                border-radius:14px;
                padding:18px;
                margin:14px 0 18px;
                box-shadow:0 2px 10px rgba(0,0,0,0.04);
            ">
              <h3 style="margin:0 0 10px;font-size:20px;color:#111827;">
                Beliefs about perception shape perceptual inference: An ideal observer model of detection.
              </h3>

              <p style='margin:0 0 12px;font-size:14px;color:#374151;line-height:1.6;'>Using near-threshold detection tasks with occlusion, the authors show that decisions about presence versus absence depend on sensory evidence and participants’ beliefs about visibility. They propose an ideal observer model that captures these effects and accounts for individual differences in perceptual inference.</p>

              <div style="font-size:13px;color:#4b5563;line-height:1.7;">
                <div><b>Authors:</b> Mazor, M., Moran, R., Press, C.</div>
                <div><b>Published:</b> 2025-03</div>
                <div><b>Relevance Score:</b> 0.4</div>
                <div style="margin-top:10px;">
                  <b>DOI:</b>
                  <a href='https://doi.org/10.1037/rev0000552' target="_blank" rel="noopener noreferrer" style='color:#2563eb;text-decoration:none;'>https://doi.org/10.1037/rev0000552</a>
                  
                </div>

                <div style="
                    margin-top:12px;
                    padding:12px;
                    background:#f9fafb;
                    border:1px solid #eef0f4;
                    border-radius:12px;
                ">
                  <b>Abstract:</b><br>
                  <span style="color:#374151;">According to Bayesian, &quot;inverse optics&quot; accounts of vision, perceiving is inferring the most likely state of the world given noisy sensory data. This inference depends not only on prior beliefs about the world but also on an internal model specifying how world states translate to visual sensations. Alternative accounts explain perceptual decisions as a rule-based process, with no role for such beliefs about perception. Here, we contrast the two alternatives, focusing on decisions about perceptual absence as a critical test case. We present data from three preregistered experiments where participants performed a near-threshold detection task under different levels of partial stimulus occlusion, thereby visibly manipulating the measurement function going from external world states to internal perceptual states. We find that decisions about presence and absence are differentially sensitive to sensory evidence and occlusion. Furthermore, we observe reliably opposite individual-level effects of occlusion on decisions about absence. Our model accounts for these findings by postulating robust individual differences in the incorporation of beliefs about visibility into perceptual inferences, independent of population variability in visibility itself. We discuss implications for the varied and inferential nature of visual perception more broadly. (PsycInfo Database Record (c) 2025 APA, all rights reserved).</span>
                </div>
              </div>
            </div>
            
        <h2 style="
            margin:34px 0 10px;
            font-size:24px;
            color:#b91c1c;
        ">
          Management Science
        </h2>
        
            <div style="
                background:#ffffff;
                border:1px solid #e5e7eb;
                border-radius:14px;
                padding:18px;
                margin:14px 0 18px;
                box-shadow:0 2px 10px rgba(0,0,0,0.04);
            ">
              <h3 style="margin:0 0 10px;font-size:20px;color:#111827;">
                Human-Algorithm Collaboration with Private Information: Naïve Advice-Weighting Behavior and Mitigation
              </h3>

              <p style='margin:0 0 12px;font-size:14px;color:#374151;line-height:1.6;'>People tend to overweight or underweight algorithmic advice when predicting demand, revealing a naive advice-weighting bias. Interventions like feature transparency and strategy training reduce prediction error.</p>

              <div style="font-size:13px;color:#4b5563;line-height:1.7;">
                <div><b>Authors:</b> Balakrishnan, M., Ferreira, K., Tong, J.</div>
                <div><b>Published:</b> 2025-03</div>
                <div><b>Relevance Score:</b> 0.39</div>
                <div style="margin-top:10px;">
                  <b>DOI:</b>
                  <a href='https://doi.org/10.1287/mnsc.2022.03850' target="_blank" rel="noopener noreferrer" style='color:#2563eb;text-decoration:none;'>https://doi.org/10.1287/mnsc.2022.03850</a>
                  
                </div>

                <div style="
                    margin-top:12px;
                    padding:12px;
                    background:#f9fafb;
                    border:1px solid #eef0f4;
                    border-radius:12px;
                ">
                  <b>Abstract:</b><br>
                  <span style="color:#374151;">Even if algorithms make better predictions than humans on average, humans may sometimes have private information that an algorithm does not have access to that can improve performance. How can we help humans effectively use and adjust recommendations made by algorithms in such situations? When deciding whether and how to override an algorithm’s recommendations, we hypothesize that people are biased toward following naïve advice-weighting (NAW) behavior; they take a weighted average between their own prediction and the algorithm’s prediction, with a constant weight across prediction instances regardless of whether they have valuable private information. This leads to humans overadhering to the algorithm’s predictions when their private information is valuable and underadhering when it is not. In an online experiment where participants were tasked with making demand predictions for 20 products while having access to an algorithm’s predictions, we confirm this bias toward NAW and find that it leads to a 20%–61% increase in prediction error. In a second experiment, we find that feature transparency—even when the underlying algorithm is a black box—helps users more effectively discriminate how to deviate from algorithms, resulting in a 25% reduction in prediction error. We make further improvements in a third experiment via an intervention designed to move users away from advice weighting and instead, use only their private information to inform deviations, leading to a 34% reduction in prediction error. This paper has been This paper was accepted by Elena Katok for the Special Issue on the Human-Algorithm Connection. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.03850 .</span>
                </div>
              </div>
            </div>
            
      </div>
    </body>
    </html>
    